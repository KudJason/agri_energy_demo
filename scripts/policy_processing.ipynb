{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Policy Document Processing\n",
                "\n",
                "This notebook handles the downloading of agricultural policy documents for Flanders, Wallonia, and France, and converts them into Markdown format for downstream LLM processing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import requests\n",
                "import time\n",
                "import re\n",
                "from bs4 import BeautifulSoup\n",
                "from urllib.parse import urljoin, unquote, urlparse\n",
                "from pypdf import PdfReader\n",
                "\n",
                "# Configuration\n",
                "DATA_DIR = \"../data\"\n",
                "PDF_DIR = os.path.join(DATA_DIR, \"pdfs\")\n",
                "MD_DIR = os.path.join(DATA_DIR, \"markdown\")\n",
                "\n",
                "USER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
                "\n",
                "os.makedirs(PDF_DIR, exist_ok=True)\n",
                "os.makedirs(MD_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sanitize_filename(text):\n",
                "    \"\"\"Creates a safe filename from URL or title.\"\"\"\n",
                "    if text.lower().endswith('.pdf'):\n",
                "        text = text[:-4]\n",
                "        \n",
                "    keepcharacters = (' ','.','_','-')\n",
                "    clean = \"\".join(c for c in text if c.isalnum() or c in keepcharacters).strip()\n",
                "    return clean.replace(\" \", \"_\") + \".pdf\"\n",
                "\n",
                "def download_file(url, folder, filename=None):\n",
                "    try:\n",
                "        if not filename:\n",
                "            path_part = urlparse(url).path\n",
                "            filename = unquote(path_part.split('/')[-1])\n",
                "        \n",
                "        if not filename.lower().endswith('.pdf'):\n",
                "            filename += \".pdf\"\n",
                "            \n",
                "        local_path = os.path.join(folder, filename)\n",
                "        \n",
                "        if os.path.exists(local_path):\n",
                "            print(f\"Skipping existing: {filename}\")\n",
                "            return local_path\n",
                "\n",
                "        print(f\"Downloading: {url}...\")\n",
                "        headers = {\"User-Agent\": USER_AGENT}\n",
                "        with requests.get(url, stream=True, headers=headers, timeout=30) as r:\n",
                "            r.raise_for_status()\n",
                "            with open(local_path, 'wb') as f:\n",
                "                for chunk in r.iter_content(chunk_size=8192):\n",
                "                    f.write(chunk)\n",
                "        print(f\"Saved to {local_path}\")\n",
                "        time.sleep(1) # Be polite\n",
                "        return local_path\n",
                "    except Exception as e:\n",
                "        print(f\"Error downloading {url}: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Downloaders by Region"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scrape_flanders():\n",
                "    print(\"\\n--- Starting Flanders Download ---\")\n",
                "    folder = os.path.join(PDF_DIR, \"flanders\")\n",
                "    os.makedirs(folder, exist_ok=True)\n",
                "    \n",
                "    target_url = \"https://www.vlaanderen.be/landbouw/glb2023\"\n",
                "    \n",
                "    try:\n",
                "        print(f\"Scraping {target_url}...\")\n",
                "        r = requests.get(target_url, headers={\"User-Agent\": USER_AGENT})\n",
                "        if r.status_code == 404:\n",
                "             print(\"Hit 404 on main link. Trying variants...\")\n",
                "             target_url = \"https://lv.vlaanderen.be/nl/glb-2023-2027\"\n",
                "             r = requests.get(target_url, headers={\"User-Agent\": USER_AGENT})\n",
                "             \n",
                "        soup = BeautifulSoup(r.text, 'html.parser')\n",
                "        links = soup.find_all('a', href=True)\n",
                "        \n",
                "        count = 0\n",
                "        for link in links:\n",
                "            href = link['href']\n",
                "            full_url = urljoin(target_url, href)\n",
                "            \n",
                "            if full_url.lower().endswith('.pdf'):\n",
                "                download_file(full_url, folder)\n",
                "                count += 1\n",
                "                if count > 5: break\n",
                "                \n",
                "        print(f\"Flanders: Found {count} docs\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Flanders Error: {e}\")\n",
                "\n",
                "def scrape_wallonia():\n",
                "    print(\"\\n--- Starting Wallonia Download ---\")\n",
                "    folder = os.path.join(PDF_DIR, \"wallonia\")\n",
                "    os.makedirs(folder, exist_ok=True)\n",
                "    print(\"Wallonia: Automated download requires authenticated/complex portal access. Skipping.\")\n",
                "\n",
                "def scrape_france():\n",
                "    print(\"\\n--- Starting France Download ---\")\n",
                "    folder = os.path.join(PDF_DIR, \"france\")\n",
                "    os.makedirs(folder, exist_ok=True)\n",
                "    \n",
                "    sources = [\n",
                "        \"https://agriculture.gouv.fr/pac-2023-2027-les-aides-du-plan-strategique-national-psn\",\n",
                "        \"https://www.telepac.agriculture.gouv.fr/telepac/html/public/aides/formulaires-2023.html\" \n",
                "    ]\n",
                "    \n",
                "    # Fallback to direct known PDF\n",
                "    fallback_urls = [\n",
                "         \"https://agriculture.gouv.fr/sites/default/files/psn-pac-2023-2027-valide_ce-31082022.pdf\"\n",
                "    ]\n",
                "    \n",
                "    found_any = False\n",
                "    \n",
                "    for url in sources:\n",
                "        try:\n",
                "            print(f\"Scraping {url}...\")\n",
                "            r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, verify=False)\n",
                "            soup = BeautifulSoup(r.text, 'html.parser')\n",
                "            links = soup.find_all('a', href=True)\n",
                "            \n",
                "            count = 0\n",
                "            for link in links:\n",
                "                full_url = urljoin(url, link['href'])\n",
                "                if full_url.lower().endswith('.pdf') and (\"psn\" in full_url or \"guide\" in full_url):\n",
                "                     download_file(full_url, folder)\n",
                "                     count += 1\n",
                "                     found_any = True\n",
                "                     if count > 5: break\n",
                "            print(f\"France: Found {count} docs at {url}\")\n",
                "        except Exception as e:\n",
                "            print(f\"France Error: {e}\")\n",
                "\n",
                "    if not found_any:\n",
                "        print(\"Trying fallback URLs...\")\n",
                "        for url in fallback_urls:\n",
                "            if download_file(url, folder):\n",
                "                print(\"Fallback successful.\")\n",
                "                break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. PDF to Markdown Conversion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def convert_pdf_to_markdown(filepath, output_dir):\n",
                "    try:\n",
                "        filename = os.path.basename(filepath)\n",
                "        name_no_ext = os.path.splitext(filename)[0]\n",
                "        output_path = os.path.join(output_dir, name_no_ext + \".md\")\n",
                "        \n",
                "        # Importing pymupdf4llm for high quality conversion\n",
                "        import pymupdf4llm\n",
                "            \n",
                "        print(f\"Converting {filename} with pymupdf4llm...\")\n",
                "        md_text = pymupdf4llm.to_markdown(filepath)\n",
                "        \n",
                "        final_content = f\"# {name_no_ext.replace('_', ' ')}\\n\\n\"\n",
                "        final_content += f\"**Source File:** `{filename}`\\n\\n---\\n\\n\"\n",
                "        final_content += md_text\n",
                "\n",
                "        with open(output_path, 'w', encoding='utf-8') as f:\n",
                "            f.write(final_content)\n",
                "            \n",
                "        print(f\"Converted: {filename} -> {output_path}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error converting {filepath}: {e}\")\n",
                "\n",
                "def batch_convert():\n",
                "    print(\"\\n--- Batch Conversion to Markdown (Improved) ---\")\n",
                "    for region in ['flanders', 'wallonia', 'france']:\n",
                "        pdf_region_dir = os.path.join(PDF_DIR, region)\n",
                "        if not os.path.exists(pdf_region_dir): continue\n",
                "        \n",
                "        md_region_dir = os.path.join(MD_DIR, region)\n",
                "        os.makedirs(md_region_dir, exist_ok=True)\n",
                "        \n",
                "        for filename in os.listdir(pdf_region_dir):\n",
                "            if filename.lower().endswith('.pdf'):\n",
                "                convert_pdf_to_markdown(os.path.join(pdf_region_dir, filename), md_region_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if __name__ == \"__main__\":\n",
                "    scrape_flanders()\n",
                "    scrape_france()\n",
                "    scrape_wallonia()\n",
                "    batch_convert()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}