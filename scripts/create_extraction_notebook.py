import json
import os

# Notebook structure
notebook = {
    "cells": [],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (scripts/venv)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

def add_cell(cell_type, source):
    notebook["cells"].append({
        "cell_type": cell_type,
        "metadata": {},
        "source": source
    })

# --- Cell 1: Title & Introduction ---
add_cell("markdown", [
    "# ÂÜú‰∏öÊîøÁ≠ñËá™Âä®ÊèêÂèñÂ∑•‰ΩúÊµÅ (Agricultural Policy Extraction Workflow)\n",
    "\n",
    "Êú¨Á¨îËÆ∞Êú¨ÂÆûÁé∞‰∫Ü‰ªéÊîøÁ≠ñÊñáÊ°£ÔºàPDF/MarkdownÔºâ‰∏≠ÊèêÂèñÁªìÊûÑÂåñËßÑÂàôÁöÑÂÆåÊï¥ÊµÅÁ®ã„ÄÇ‰∏∫‰∫ÜÈÅøÂÖçÈáçÂ§çÂ∑•‰ΩúÔºåÊµÅÁ®ãÂàÜ‰∏∫Âõõ‰∏™Èò∂ÊÆµÔºåÊØè‰∏™Èò∂ÊÆµÈÉΩÊúâ**Êñá‰ª∂ÁïôÂ≠ò (Checkpoints)**„ÄÇ\n",
    "\n",
    "### ‰ºòÂåñÁâπÊÄß (Optimizations)\n",
    "1. **Âπ∂Ë°åÂ§ÑÁêÜ (Parallel Processing)**: ‰ΩøÁî®Â§öÁ∫øÁ®ã simultaneous Â§ÑÁêÜ (Max 3 workers) ‰ª•Âä†ÈÄüËØÑÂàÜÂíåÊèêÂèñ„ÄÇ\n",
    "2. **Êñ≠ÁÇπÁª≠‰º† (Resumable)**: Ëá™Âä®Ë∑≥ËøáÂ∑≤Â§ÑÁêÜÁöÑÊñá‰ª∂„ÄÇ\n",
    "\n",
    "### Èò∂ÊÆµÊ¶ÇËßà\n",
    "1.  **ËØÑÂàÜ (Scoring)**: ‰ΩøÁî® LLM ÂØπÊâÄÊúâÊñáÊ°£ÁöÑÁõ∏ÂÖ≥ÊÄßÊâìÂàÜ (0-10)„ÄÇÁªìÊûú‰øùÂ≠ò‰∏∫ `knowledge_base/scores.csv`„ÄÇ\n",
    "2.  **Á≠õÈÄâ (Selection)**: Ê†πÊçÆÂàÜÊï∞ÈÄâÂá∫ Top 20% ÁöÑÈ´ò‰ª∑ÂÄºÊñá‰ª∂„ÄÇÁªìÊûú‰øùÂ≠ò‰∏∫ `knowledge_base/selected_files.json`„ÄÇ\n",
    "3.  **ÊèêÂèñ (Extraction)**: ÂØπÁ≠õÈÄâÂá∫ÁöÑÊñá‰ª∂ËøõË°åËØ¶ÁªÜËßÑÂàôÊèêÂèñ„ÄÇÊØè‰∏™Êñá‰ª∂ÁîüÊàêÁã¨Á´ãÁöÑ TTL Êñá‰ª∂‰øùÂ≠òÂú® `knowledge_base/intermediate/`„ÄÇ\n",
    "4.  **ÂêàÂπ∂ (Merging)**: Â∞ÜÊâÄÊúâ‰∏≠Èó¥ TTL Êñá‰ª∂ÂêàÂπ∂‰∏∫ÊúÄÁªàÂõæË∞± `knowledge_base/rules_combined.ttl`„ÄÇ\n"
])

# --- Cell 2: Imports & Setup ---
add_cell("code", [
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import glob\n",
    "import re\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "from rdflib.namespace import XSD, RDFS\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- 1. ÁéØÂ¢ÉÈÖçÁΩÆ (Configuration) ---\n",
    "\n",
    "# Âä†ËΩΩÁéØÂ¢ÉÂèòÈáè (Load API Keys)\n",
    "load_dotenv()\n",
    "load_dotenv(\"/Users/jasonjia/Documents/industry_policy/secret/.env\")\n",
    "\n",
    "API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"‚ùå Êú™ÊâæÂà∞ DEEPSEEK_API_KEYÔºåËØ∑Ê£ÄÊü• .env Êñá‰ª∂ÔºÅ\")\n",
    "\n",
    "print(f\"‚úÖ API Key Loaded. Using DeepSeek API.\")\n",
    "client = OpenAI(api_key=API_KEY, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# Âπ∂Ë°åÈÖçÁΩÆ (Parallel Config)\n",
    "MAX_WORKERS = 3\n",
    "CSV_LOCK = threading.Lock()\n",
    "\n",
    "# ÂÆö‰πâË∑ØÂæÑ (Paths)\n",
    "# Use parent directory as BASE_DIR since notebook is in scripts/\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if not os.path.exists(os.path.join(BASE_DIR, \"data\")):\n",
    "    # Fallback if running from root or other context\n",
    "    BASE_DIR = os.getcwd()\n",
    "\n",
    "print(f\"üìÇ BASE_DIR: {BASE_DIR}\")\n",
    "\n",
    "DATA_DIRS = [\n",
    "    # Markdown Sources (Converted from PDF)\n",
    "    os.path.join(BASE_DIR, \"data/markdown/belgium/wallonia\"),\n",
    "    os.path.join(BASE_DIR, \"data/markdown/belgium/flanders\"),\n",
    "    os.path.join(BASE_DIR, \"data/markdown/flanders\"),\n",
    "    os.path.join(BASE_DIR, \"data/markdown/france/PSN_2023_2027\"),\n",
    "    os.path.join(BASE_DIR, \"data/markdown/france/Rules_Transversales\"),\n",
    "    os.path.join(BASE_DIR, \"data/markdown/france/TelePAC_Forms_2025\"),\n",
    "    # Existing Markdown\n",
    "    os.path.join(BASE_DIR, \"data/tocheck/wallonia/extra\")\n",
    "]\n",
    "\n",
    "for d in DATA_DIRS:\n",
    "    exists = \"‚úÖ\" if os.path.exists(d) else \"‚ùå\"\n",
    "    print(f\"{exists} Checking dir: {d}\")\n",
    "\n",
    "# ËæìÂá∫Ë∑ØÂæÑ (Output Paths)\n",
    "KB_DIR = os.path.join(BASE_DIR, \"knowledge_base\")\n",
    "INTERMEDIATE_DIR = os.path.join(KB_DIR, \"intermediate\")\n",
    "SCORES_FILE = os.path.join(KB_DIR, \"scores.csv\")\n",
    "SELECTED_FILE = os.path.join(KB_DIR, \"selected_files.json\")\n",
    "FINAL_TTL = os.path.join(KB_DIR, \"rules_combined.ttl\")\n",
    "\n",
    "os.makedirs(INTERMEDIATE_DIR, exist_ok=True)\n",
    "\n",
    "CORE = Namespace(\"http://example.org/agri-energy/core#\")\n"
])

# --- Cell 3: Helper Functions ---
add_cell("code", [
    "# --- 2. ËæÖÂä©ÂáΩÊï∞ (Helper Functions) ---\n",
    "\n",
    "def get_file_content(filepath):\n",
    "    \"\"\"Reads text from a Markdown file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading MD {os.path.basename(filepath)}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_batch_scores(file_batch):\n    \"\"\"\n    Batch score a list of (filename, text) tuples.\n    Returns a dict {filename: score}.\n    \"\"\"\n    if not file_batch:\n        return {}\n        \n    # Prepare context\n    context = \"\"\n    for i, (fname, text) in enumerate(file_batch):\n        context += f\"--- DOCUMENT {i+1}: {fname} ---\\n{text[:200]}\\n\\n\"\n        \n    prompt = f\"\"\"\n    Task: Rank the following documents based on their relevance for extracting structured rules about Agricultural Financial Aids/Subsidies.\n    \n    Criteria:\n    - High Score (8-10): Explicitly DEFINES a subsidy/intervention (eligibility, amounts, conditions).\n    - Low Score (0-3): Forms, Mandates, Receipts, Administrative procedures.\n    - **Language Note**: Process English, French, and Dutch (Flanders) documents appropriately.\n    \n    Documents:\n    {context}\n    \n    Output:\n    Return a JSON object with a list of scores:\n    {{\n      \"scores\": [\n        {{\"filename\": \"...\", \"score\": 8, \"reason\": \"...\"}},\n        ...\n      ]\n    }}\n    \"\"\"\n    \n    try:\n        response = client.chat.completions.create(\n            model=\"deepseek-chat\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n        )\n        content = response.choices[0].message.content.strip()\n        if \"```json\" in content:\n            content = content.split(\"```json\")[1].split(\"```\")[0]\n        elif \"```\" in content:\n            content = content.split(\"```\")[1].split(\"```\")[0]\n            \n        data = json.loads(content)\n        results = {}\n        for item in data.get(\"scores\", []):\n            results[item[\"filename\"]] = int(item[\"score\"])\n        return results\n    except Exception as e:\n        print(f\"‚ùå Error batch scoring: {e}\")\n        return {}\n\ndef validate_date(date_str):\n    \"\"\"Returns valid YYYY-MM-DD string or None.\"\"\"\n    if not date_str or not isinstance(date_str, str):\n        return None\n    # Simple regex for YYYY-MM-DD\n    if re.match(r\"^\\d{4}-\\d{2}-\\d{2}$\", date_str):\n        return date_str\n    return None\n"
])

# --- Cell 4: Phase 1 - Scoring ---
add_cell("markdown", [
    "### Phase 1: ËØÑÂàÜ (Scoring)\n",
    "Ê≠§Ê≠•È™§Êâ´ÊèèÊâÄÊúâÊñá‰ª∂Âπ∂ËÆ°ÁÆóÂàÜÊï∞„ÄÇÁªìÊûú‰øùÂ≠òÂà∞ `knowledge_base/scores.csv`„ÄÇ\n",
    "- **Batch Mode**: Â∞ÜÊâÄÊúâÊñáÊ°£È¢ÑËßàÊâìÂåÖÂèëÈÄÅÁªô LLM (20‰∏™/Êâπ)Ôºå‰∏ÄÊ¨°ÊÄßËé∑ÂèñÊéíÂ∫èÁªìÊûúÔºåÊèêÈ´òÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ\n",
    "- **Êñ≠ÁÇπÁª≠‰º†**: Â¶ÇÊûúÊñá‰ª∂Â∑≤ÁªèÂú® CSV ‰∏≠Ôºå‰ºöËá™Âä®Ë∑≥Ëøá„ÄÇ\n",
    "- **ËæìÂá∫**: `scores.csv`"
])
add_cell("code", [
    "# --- Phase 1: Scoring and Ranking (Batch Mode) ---\n\n# 1. ËØªÂèñÂ∑≤ÊúâÁöÑËØÑÂàÜ (Load existing scores)\nexisting_scores = {}\nif os.path.exists(SCORES_FILE):\n    with open(SCORES_FILE, 'r', encoding='utf-8') as f:\n        reader = csv.reader(f)\n        try:\n            next(reader, None) # Skip header\n            for row in reader:\n                if row:\n                    existing_scores[row[0]] = int(row[1])\n        except csv.Error:\n            pass\n    print(f\"üìä Loaded {len(existing_scores)} existing scores.\")\n\n# 2. Êî∂ÈõÜÂæÖÂ§ÑÁêÜÊñá‰ª∂ (Gather Files)\nall_files = [] # List of (filename, filepath)\nfor d in DATA_DIRS:\n    if not os.path.exists(d):\n        continue\n    for f in os.listdir(d):\n        if f.endswith(\".pdf\") or f.endswith(\".md\"):\n            if f not in existing_scores:\n                all_files.append((f, os.path.join(d, f)))\n\nprint(f\"üîç Found {len(all_files)} new files to process.\")\n\n# 3. ÊâπÈáèÂ§ÑÁêÜ (Batch Processing)\nBATCH_SIZE = 100\n\nif all_files:\n    with open(SCORES_FILE, 'a', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        if os.path.getsize(SCORES_FILE) == 0:\n            writer.writerow([\"filename\", \"score\", \"filepath\"])\n        \n        # Process in chunks\n        for i in range(0, len(all_files), BATCH_SIZE):\n            chunk = all_files[i:i + BATCH_SIZE]\n            print(f\"Processing Batch {i//BATCH_SIZE + 1} ({len(chunk)} files)...\")\n            \n            # Load content\n            batch_data = []\n            file_map = {} # filename -> filepath\n            for fname, fpath in chunk:\n                if not fpath: \n                    print(f\"[WARN] Skipping {fname}: No filepath found.\")\n                    continue\n                    \n                text = get_file_content(fpath)\n                if text:\n                    batch_data.append((fname, text))\n                    file_map[fname] = fpath\n            \n            if not batch_data: continue\n            \n            # Call LLM\n            scores = get_batch_scores(batch_data)\n            \n            # Save Results\n            for fname, score in scores.items():\n                fpath = file_map.get(fname, \"\")\n                print(f\"  ‚≠êÔ∏è {fname}: {score}\")\n                writer.writerow([fname, score, fpath])\n            f.flush()\n            \nprint(\"‚úÖ Phase 1 Complete. Scores saved to CSV.\")\n"
])

# --- Cell 5: Phase 2 - Selection ---
add_cell("markdown", [
    "### Phase 2: Á≠õÈÄâ (Selection)\n",
    "Ê≠§Ê≠•È™§Ê†πÊçÆ CSV ‰∏≠ÁöÑÂàÜÊï∞ÔºåÁ≠õÈÄâÂá∫ Top 20% ÁöÑÊñá‰ª∂„ÄÇ\n",
    "- **ËæìÂá∫**: `knowledge_base/selected_files.json` (‰Ωú‰∏∫ÂêéÁª≠ÊèêÂèñÁöÑÂæÖÂäûÂàóË°®)"
])
add_cell("code", [
    "# --- Phase 2: Selection ---\n",
    "\n",
    "# 1. ËØªÂèñÊâÄÊúâÂàÜÊï∞\n",
    "all_files = []\n",
    "with open(SCORES_FILE, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            row['score'] = int(row['score'])\n",
    "            all_files.append(row)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "# 2. ÊéíÂ∫è\n",
    "all_files.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "# 3. Á≠õÈÄâ (Filter Score >= 5)\n# Strategy: Select all files with score >= 5. If fewer than 5 files meet this, take top 5.\nselected_files = [f for f in all_files if f['score'] >= 5]\n\nif len(selected_files) < 5:\n    print(f\"‚ö†Ô∏è Only {len(selected_files)} files have score >= 5. Selecting top 5 fallback.\")\n    selected_files = all_files[:5]\n",
    "\n",
    "print(f\"üìä Total Files Scored: {len(all_files)}\")\n",
    "print(f\"‚úÖ Selected {len(selected_files)} files for extraction.\")\n",
    "\n",
    "for f in selected_files:\n",
    "    print(f\"  - [{f['score']}] {f['filename']}\")\n",
    "\n",
    "# 4. ‰øùÂ≠òÁ≠õÈÄâÁªìÊûú\n",
    "with open(SELECTED_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(selected_files, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "print(f\"üíæ Selection saved to {SELECTED_FILE}\")"
])

# --- Cell 6: Phase 3 - Granular Extraction (Function Definition) ---
add_cell("code", [
    "def extract_granular_rules_perplexity(text, filename):\n",
    "    \"\"\"\n",
    "    Main Extraction Prompt.\n",
    "    ÊèêÂèñÊèêÁ§∫ËØçÊ†∏ÂøÉÈÄªËæë„ÄÇ\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an Expert Legal Policy Analyst for the Agricultural Sector.\n",
    "    \n",
    "    OBJECTIVE:\n",
    "    Analyze the provided **Multiple Policy Documents** and extract structured **Financial Opportunities**.\n",
    "    \n",
    "    context_mapping:\n",
    "    - **YoungFarmer**: \"Jeune Agriculteur\", \"Jonge Landbouwer\"\n",
    "    - **ActiveFarmer**: \"Agriculteur Actif\", \"Actieve Landbouwer\"\n",
    "    - **OrganicFarmer**: \"Agriculteur Bio\", \"Bio\", \"Organic\"\n",
    "    \n",
    "    CRITICAL FILTERING RULES:\n",
    "    1. **IGNORE** administrative procedures and general definitions.\n",
    "    2. **AVOID DUPLICATES within valid output**: If the same measure appears in multiple files with the SAME code, consistent with the Policy Framework, merge or extract unique aspects.\n",
    "    3. **REQUIRE UNIQUE IDENTIFIER**: Official code (e.g., \"MB13\", \"Ecoregime1\").\n",
    "    \n",
    "    OUTPUT FORMAT (JSON):\n",
    "    Return a JSON object with a single key \"opportunities\" containing a LIST of objects.\n",
    "    **IMPORTANT**: You must indicate which `source_filename` each opportunity came from.\n",
    "    \n",
    "    {{\n",
    "      \"source_filename\": \"Name of the file this rule was found in\",\n",
    "      \"official_code\": \"Unique Code or N/A\",\n",
    "      \"name_local\": \"Official Name\",\n",
    "      \"name_en\": \"English Translation\",\n",
    "      \"region\": \"Region (France, Wallonia, Flanders, EU)\",\n",
    "      \"version_year\": 2024,\n",
    "      \"replaces_code\": \"Code of previous version if mentioned\",\n",
    "      \n",
    "      \"farmer_types\": [\"YoungFarmer\", \"ActiveFarmer\", \"NewEntrant\", \"SmallFarmer\", \"OrganicFarmer\"],\n",
    "      \n",
    "      \"payment_rate\": 120.5 (Float),\n",
    "      \"payment_unit\": \"EUR/ha\",\n",
    "      \"payment_limit\": \"Max 50ha\",\n",
    "      \n",
    "      \"benefit_amount\": \"Narrative\",\n",
    "      \"calculation_logic\": \"Formula\",\n",
    "      \"description\": \"Summary\",\n",
    "      \"valid_from\": \"Year\",\n",
    "      \"application_start_date\": \"YYYY-MM-DD\",\n",
    "      \"application_end_date\": \"YYYY-MM-DD\",\n",
    "      \"eligibility_criteria\": [ {{\"variable\": \"...\", \"operator\": \"...\", \"value\": \"...\"}} ]\n",
    "    }}\n",
    "    \n",
    "    DOCUMENTS TO ANALYZE:\n",
    "    {text}\n",
    "    \n",
    "    ONLY JSON.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-reasoner\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        # Cleanup Markdown\n",
    "        if \"```json\" in content:\n",
    "            content = content.split(\"```json\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in content:\n",
    "            content = content.split(\"```\")[1].split(\"```\")[0]\n",
    "            \n",
    "        return json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error LLM extraction {filename}: {e}\")\n",
    "        return {\"opportunities\": []}\n"
])

# --- Cell 7: Phase 3 - Execution ---
add_cell("markdown", [
    "### Phase 3: ÊèêÂèñ (Extraction)\n",
    "ÂØπÁ≠õÈÄâÂá∫ÁöÑÊØè‰∏™Êñá‰ª∂ËøõË°åËØ¶ÁªÜÂ§ÑÁêÜ„ÄÇ\n",
    "- **Âπ∂Ë°åÂ§ÑÁêÜ**: ‰ΩøÁî® `ThreadPoolExecutor` (Max Workers=3) Âπ∂Ë°åÊâßË°å„ÄÇ\n",
    "- **Checkpoints**: ÊØè‰∏™Êñá‰ª∂Áã¨Á´ãÁîüÊàê TTL„ÄÇ\n"
])
add_cell("code", [
    "# --- Phase 3: Extraction Execution (Parallelized) ---\n",
    "\n",
    "with open(SELECTED_FILE, 'r', encoding='utf-8') as f:\n",
    "    files_to_extract = json.load(f)\n",
    "\n",
    "print(f\"üöÄ Starting extraction for {len(files_to_extract)} files...\")\n",
    "\n",
    "# Group files into batches for LLM processing\n",
    "EXTRACTION_BATCH_SIZE = 5 # Process 5 files at a time by LLM\n",
    "batches = [files_to_extract[i:i + EXTRACTION_BATCH_SIZE] for i in range(0, len(files_to_extract), EXTRACTION_BATCH_SIZE)]\n",
    "\n",
    "def process_extraction_task(item_batch):\n",
    "    # 1. Check Checkpoint (Skip if ANY of the batch is done? No, strictly check output)\n",
    "    # Strategy: We generate one TTL per BATCH named 'batch_{hash}.ttl' or handling per-file logic is complex.\n",
    "    # BETTER: We pass the batch to LLM, but inside the LLM prompt we asked for source_filename.\n",
    "    \n",
    "    # Batch ID based on first filename to keep it deterministic\n",
    "    batch_id = item_batch[0]['filename'].replace(\".pdf\", \"\").replace(\".md\", \"\").replace(\" \", \"_\").lower()\n",
    "    ttl_filename = f\"batch_{batch_id}.ttl\"\n",
    "    ttl_path = os.path.join(INTERMEDIATE_DIR, ttl_filename)\n",
    "    \n",
    "    if os.path.exists(ttl_path):\n",
    "        print(f\"‚è≠Ô∏è Skipping Batch {batch_id} (Already extracted).\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Processing Batch {batch_id} ({len(item_batch)} files)...\")\n",
    "    \n",
    "    # Concatenate Text\n",
    "    combined_text = \"\"\n",
    "    for f in item_batch:\n",
    "        f_text = get_file_content(f['filepath'])\n",
    "        if f_text:\n",
    "             combined_text += f\"\\n\\n=== DOCUMENT START: {f['filename']} ===\\n{f_text[:20000]} \\n=== DOCUMENT END ===\\n\" # Cap each file to 20k chars\n",
    "    \n",
    "    if not combined_text: return\n",
    "        \n",
    "    # 2. Extract\n",
    "    data = extract_granular_rules_perplexity(combined_text, f\"Batch {batch_id}\")\n",
    "\n",
    "    # 3. RDF Generation\n",
    "    g = Graph()\n",
    "    g.bind(\"core\", CORE)\n",
    "    \n",
    "    opps = data.get(\"opportunities\", [])\n",
    "    if not opps:\n",
    "        # Save empty marker\n",
    "        with open(ttl_path, 'w') as f: f.write(\"# No opportunities found\")\n",
    "        print(f\"  üíæ Saved empty marker: {batch_id}\")\n",
    "        return\n",
    "\n",
    "    print(f\"  ‚úÖ Found {len(opps)} opportunities in batch.\")\n",
    "    for idx, opp in enumerate(opps):\n",
    "        source_file = opp.get(\"source_filename\", \"unknown_batch_file\")\n",
    "        safe_name = source_file.replace(\".pdf\", \"\").replace(\".md\", \"\").replace(\" \", \"_\").lower()\n",
    "        \n",
    "        code = opp.get(\"official_code\", \"N/A\")\n",
    "        \n",
    "        # Deduplication Strategy: Deterministic URI based on Code\n",
    "        if code and code != \"N/A\":\n",
    "            safe_code = \"\".join(c if c.isalnum() else \"_\" for c in code).lower()\n",
    "            opp_uri = URIRef(f\"http://example.org/opportunity/code/{safe_code}\")\n",
    "        else:\n",
    "            # Fallback: Hash of name + file\n",
    "            safe_id = f\"{safe_name}_{idx}\"\n",
    "            opp_uri = URIRef(f\"http://example.org/opportunity/{safe_id}\")\n",
    "            \n",
    "        doc_uri = URIRef(f\"http://example.org/doc/{safe_name}\")\n",
    "        g.add((opp_uri, RDF.type, CORE.Opportunity))\n",
    "        g.add((opp_uri, CORE.definedBy, doc_uri))\n",
    "        \n",
    "        if code != \"N/A\": g.add((opp_uri, CORE.hasInterventionCode, Literal(code)))\n",
    "        if opp.get(\"name_local\"): g.add((opp_uri, RDFS.label, Literal(opp[\"name_local\"], lang=\"fr\")))\n",
    "        if opp.get(\"name_en\"): g.add((opp_uri, RDFS.label, Literal(opp[\"name_en\"], lang=\"en\")))\n",
    "        if opp.get(\"description\"): g.add((opp_uri, CORE.description, Literal(opp[\"description\"], lang=\"en\")))\n",
    "        if opp.get(\"region\"): g.add((opp_uri, CORE.hasRegion, Literal(opp[\"region\"])))\n",
    "\n",
    "        # --- Enhanced Fields (Ontology Strategy) ---\n",
    "        if opp.get(\"version_year\"): g.add((opp_uri, CORE.versionYear, Literal(opp[\"version_year\"], datatype=XSD.integer)))\n",
    "        \n",
    "        # Payment Structure\n",
    "        if opp.get(\"payment_rate\") and isinstance(opp[\"payment_rate\"], (int, float)):\n",
    "             g.add((opp_uri, CORE.paymentRate, Literal(opp[\"payment_rate\"], datatype=XSD.float)))\n",
    "        if opp.get(\"payment_unit\"): g.add((opp_uri, CORE.paymentUnit, Literal(opp[\"payment_unit\"])))\n",
    "        if opp.get(\"payment_limit\"): g.add((opp_uri, CORE.paymentLimit, Literal(opp[\"payment_limit\"])))\n",
    "\n",
    "        # Rich Text Fallbacks\n",
    "        if opp.get(\"benefit_amount\"): g.add((opp_uri, CORE.benefitAmount, Literal(opp[\"benefit_amount\"])))\n",
    "        if opp.get(\"calculation_logic\"): g.add((opp_uri, CORE.calculationLogic, Literal(opp[\"calculation_logic\"])))\n",
    "        if opp.get(\"application_process\"): g.add((opp_uri, CORE.applicationRule, Literal(opp[\"application_process\"])))\n",
    "        \n",
    "        start_date = validate_date(opp.get(\"application_start_date\"))\n",
    "        if start_date: \n",
    "            g.add((opp_uri, CORE.applicationStartDate, Literal(start_date, datatype=XSD.date)))\n",
    "            \n",
    "        end_date = validate_date(opp.get(\"application_end_date\"))\n",
    "        if end_date: \n",
    "            g.add((opp_uri, CORE.applicationEndDate, Literal(end_date, datatype=XSD.date)))\n",
    "\n",
    "        for tag in opp.get(\"tags\", []):\n",
    "            g.add((opp_uri, CORE.hasTag, Literal(tag)))\n",
    "                 \n",
    "        # Schema mappings\n",
    "        for ft in opp.get(\"farmer_types\", []):\n",
    "            if ft in [\"YoungFarmer\", \"ActiveFarmer\", \"NewEntrant\", \"SmallFarmer\", \"OrganicFarmer\"]:\n",
    "                g.add((opp_uri, CORE.eligibleFarmerType, URIRef(CORE + ft)))\n",
    "            else:\n",
    "                 g.add((opp_uri, CORE.eligibleFarmerType, Literal(ft)))\n",
    "                 \n",
    "        for lu in opp.get(\"land_uses\", []):\n",
    "            if lu in [\"ArableLand\", \"PermanentPasture\", \"PermanentCrops\", \"Natura2000\"]:\n",
    "                g.add((opp_uri, CORE.eligibleLandUse, URIRef(CORE + lu)))\n",
    "            else:\n",
    "                g.add((opp_uri, CORE.eligibleLandUse, Literal(lu)))\n",
    "\n",
    "        if opp.get(\"min_age\"): g.add((opp_uri, CORE.minAge, Literal(int(opp[\"min_age\"]))))\n",
    "        if opp.get(\"max_age\"): g.add((opp_uri, CORE.maxAge, Literal(int(opp[\"max_age\"]))))\n",
    "        if opp.get(\"min_hectares\"): g.add((opp_uri, CORE.minHectares, Literal(float(opp[\"min_hectares\"]))))\n",
    "            \n",
    "    g.serialize(destination=ttl_path, format=\"turtle\")\n",
    "    print(f\"  üíæ Saved {ttl_filename}\")\n",
    "\n",
    "# Execute\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    list(executor.map(process_extraction_task, batches))\n",
    "    \n",
    "print(\"‚úÖ All extraction tasks completed.\")"
])

# --- Cell 8: Phase 4 - Merging ---
add_cell("markdown", [
    "### Phase 4: ÂêàÂπ∂ (Merging)\n",
    "Â∞Ü `knowledge_base/intermediate/` ÁõÆÂΩï‰∏ãÁöÑÊâÄÊúâ TTL Êñá‰ª∂ÂêàÂπ∂‰∏∫‰∏Ä‰∏™ÊúÄÁªàÁöÑÂõæË∞±„ÄÇ\n",
    "- **ËæìÂá∫**: `knowledge_base/rules_combined.ttl`"
])
add_cell("code", [
    "# --- Phase 4: Merge Graphs ---\n",
    "\n",
    "combined_g = Graph()\n",
    "combined_g.bind(\"core\", CORE)\n",
    "\n",
    "ttl_files = glob.glob(os.path.join(INTERMEDIATE_DIR, \"*.ttl\"))\n",
    "print(f\"üì¶ Merging {len(ttl_files)} TTL files...\")\n",
    "\n",
    "for ttl_file in ttl_files:\n",
    "    try:\n",
    "        combined_g.parse(ttl_file, format=\"turtle\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error parsing {os.path.basename(ttl_file)}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Final Graph contains {len(combined_g)} triples.\")\n",
    "combined_g.serialize(destination=FINAL_TTL, format=\"turtle\")\n",
    "print(f\"üéâ Saved merged graph to {FINAL_TTL}\")"
])

# Write notebook file (to scripts directory)
output_path = "/Users/jasonjia/Documents/industry_policy/agri_energy_demo/scripts/policy_extraction_workflow.ipynb"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(notebook, f, indent=2, ensure_ascii=False)

print(f"Notebook created successfully at {output_path}")
